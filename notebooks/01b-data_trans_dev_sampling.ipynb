{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "    # Get env variables\n",
    "    uri = os.getenv(\"NEO4J_URI\")\n",
    "    user = os.getenv(\"NEO4J_USERNAME\")\n",
    "    password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "    return GraphDatabase.driver(uri, auth=(user, password),\n",
    "                                encrypted=False,\n",
    "                                max_connection_lifetime=3600)\n",
    "\n",
    "driver = initialize_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2021-03-01\n",
      "2 2021-04-01\n",
      "3 2021-05-01\n",
      "4 2021-06-01\n",
      "5 2021-07-01\n",
      "6 2021-08-01\n",
      "7 2021-09-01\n",
      "8 2021-10-01\n"
     ]
    }
   ],
   "source": [
    "# Set periods for iteration\n",
    "OBSERVATION_START = '2021-02-01'  # First date of observation period\n",
    "DATE_START = '2021-03-01'  # Date of the first snapshot\n",
    "DATE_END = '2021-10-01'  # Date of the last snapshot\n",
    "PERIOD_LENGTH = relativedelta(months=1)  # Time between snapshots\n",
    "\n",
    "def create_observations():\n",
    "    period = DATE_START\n",
    "    periods = []\n",
    "    obs = 0\n",
    "    while period <= DATE_END:\n",
    "        periods.append(period)\n",
    "        next_period = (parse(period) + PERIOD_LENGTH).strftime(\"%Y-%m-%d\")\n",
    "        period = next_period\n",
    "        obs += 1\n",
    "\n",
    "    observations = periods\n",
    "\n",
    "    [print(i+1, obs) for i, obs in enumerate(observations)]\n",
    "\n",
    "    return observations\n",
    "\n",
    "observations = create_observations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling & Nodelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_packages(driver, date):\n",
    "    with driver.session(database='main') as session:\n",
    "        query = \"\"\"\n",
    "                MATCH (pa:Package)-[r:DEVELOPED_AT]->(pr:Project)\n",
    "                WITH pr, COUNT(r) AS num_pkgs\n",
    "                WHERE num_pkgs = 1\n",
    "                WITH pr\n",
    "                MATCH (pa:Package)-[r:DEVELOPED_AT]->(pr)\n",
    "                WHERE pa.created < DateTime($date)\n",
    "                RETURN pa.name AS name,\n",
    "                       pa.repo_name AS repo_name,\n",
    "                       pa.repo_owner AS repo_owner,\n",
    "                       toString(pa.created) AS created\n",
    "                \"\"\"\n",
    "        \n",
    "        results = session.run(query, date=date).data()\n",
    "    \n",
    "        packages = pd.DataFrame.from_dict(results)\n",
    "        packages['created'] = pd.to_datetime(packages['created'])\n",
    "        packages['observation'] = pd.to_datetime(date)\n",
    "    \n",
    "    return packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_developers(driver, repository, start, end):\n",
    "    with driver.session(database='main') as session:\n",
    "        query_comments = \"\"\"\n",
    "            MATCH (u:User)-[p:POSTED]->(c:Comment)-[*2]->(r:Repository)\n",
    "            WHERE r.name = toString($repository)\n",
    "            AND c.created >= datetime($start)\n",
    "            AND c.created < datetime($end)\n",
    "            AND u.type <> \"Bot\"\n",
    "            AND NOT u.login CONTAINS \"[bot]\"\n",
    "            RETURN DISTINCT u.login AS login\n",
    "            \"\"\"\n",
    "                \n",
    "        query_issues = \"\"\"\n",
    "            MATCH (u:User)-[a]->(:Issue)-[:RELATED_TO]->(r:Repository)\n",
    "            WHERE r.name = toString($repository)\n",
    "            AND a.created >= datetime($start)\n",
    "            AND a.created < datetime($end)\n",
    "            AND u.type <> \"Bot\"\n",
    "            AND NOT u.login CONTAINS \"[bot]\"\n",
    "            RETURN DISTINCT u.login AS login\n",
    "            \"\"\"\n",
    "\n",
    "        query_pullreq = \"\"\"\n",
    "            MATCH (u:User)-[a]->(:PullRequest)-[:RELATED_TO]->(r:Repository)\n",
    "            WHERE r.name = toString($repository)\n",
    "            AND a.created >= datetime($start)\n",
    "            AND a.created < datetime($end)\n",
    "            AND u.type <> \"Bot\"\n",
    "            AND NOT u.login CONTAINS \"[bot]\"\n",
    "            RETURN DISTINCT u.login AS login\n",
    "            \"\"\"\n",
    "\n",
    "        query_commits = \"\"\"\n",
    "            MATCH (u:User)-[a]->(:Commit)-[:RELATED_TO]->(r:Repository)\n",
    "            WHERE r.name = toString($repository)\n",
    "            AND a.created >= datetime($start)\n",
    "            AND a.created < datetime($end)\n",
    "            AND u.type <> \"Bot\"\n",
    "            AND NOT u.login CONTAINS \"[bot]\"\n",
    "            RETURN DISTINCT u.login AS login\n",
    "            \"\"\"\n",
    "\n",
    "        results_comments = session.run(\n",
    "            query_comments, repository=repository, start=start, end=end).data()\n",
    "        results_issues = session.run(\n",
    "            query_issues, repository=repository, start=start, end=end).data()\n",
    "        results_pullreq = session.run(\n",
    "            query_pullreq, repository=repository, start=start, end=end).data()\n",
    "        results_commits = session.run(\n",
    "            query_commits, repository=repository, start=start, end=end).data()\n",
    "\n",
    "        comments = pd.DataFrame.from_dict(results_comments)        \n",
    "        issues = pd.DataFrame.from_dict(results_issues)\n",
    "        pullreqs = pd.DataFrame.from_dict(results_pullreq)\n",
    "        commits = pd.DataFrame.from_dict(results_commits)\n",
    "\n",
    "        developers = pd.concat([comments, issues, pullreqs, commits], axis=0, ignore_index=True)\n",
    "        developers.drop_duplicates(subset=['login'], inplace=True)\n",
    "\n",
    "    return developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ef6aabd0d945e9afc8fa4c808361f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get list of packages created until start of observation\n",
    "def get_developer_sample():\n",
    "    packages = get_packages(driver, OBSERVATION_START)\n",
    "    repo_names = packages['repo_name'].tolist()\n",
    "    developers = []\n",
    "    for repo in tqdm(repo_names):\n",
    "        repo_devs = get_developers(driver, repo, OBSERVATION_START, DATE_END)\n",
    "        developers.extend(repo_devs.to_dict(orient=\"records\"))\n",
    "\n",
    "    devs = pd.DataFrame.from_records(developers)\n",
    "    devs_sample = devs.drop_duplicates(subset=['login'])\n",
    "    devs_sample = devs_sample.sample(n=SAMPLE_SIZE, random_state=17, ignore_index=True)\n",
    "\n",
    "    logins_sample = devs_sample['login'].unique().tolist()\n",
    "    nodelist_devs = [\"_\".join([\"dev\", name]) for name in logins_sample]\n",
    "\n",
    "    nodelist = pd.DataFrame(nodelist_devs, columns=['id'])\n",
    "    nodelist.to_csv('../data/nodelists/developers.csv', index=False)\n",
    "    \n",
    "    return devs_sample, nodelist_devs\n",
    "    \n",
    "sample_devs, nodelist_devs = get_developer_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_developer_affiliations(developer, start, end):\n",
    "    with driver.session(database='main') as session:\n",
    "        query_comments = \"\"\"\n",
    "            MATCH (u:User)-[p:POSTED]->(c:Comment)-[*2]->(r:Repository)\n",
    "            WHERE u.login = toString($developer)\n",
    "            AND c.created >= datetime($start)\n",
    "            AND c.created < datetime($end)\n",
    "            RETURN DISTINCT r.name AS repo_name\n",
    "            \"\"\"\n",
    "                \n",
    "        query_issues = \"\"\"\n",
    "            MATCH (u:User)-[a]->(:Issue)-[:RELATED_TO]->(r:Repository)\n",
    "            WHERE u.login = toString($developer)\n",
    "            AND a.created >= datetime($start)\n",
    "            AND a.created < datetime($end)\n",
    "            RETURN DISTINCT r.name AS repo_name\n",
    "            \"\"\"\n",
    "\n",
    "        query_pullreq = \"\"\"\n",
    "            MATCH (u:User)-[a]->(:PullRequest)-[:RELATED_TO]->(r:Repository)\n",
    "            WHERE u.login = toString($developer)\n",
    "            AND a.created >= datetime($start)\n",
    "            AND a.created < datetime($end)\n",
    "            RETURN DISTINCT r.name AS repo_name\n",
    "            \"\"\"\n",
    "\n",
    "        query_commits = \"\"\"\n",
    "            MATCH (u:User)-[a]->(:Commit)-[:RELATED_TO]->(r:Repository)\n",
    "            WHERE u.login = toString($developer)\n",
    "            AND a.created >= datetime($start)\n",
    "            AND a.created < datetime($end)\n",
    "            RETURN DISTINCT r.name AS repo_name\n",
    "            \"\"\"\n",
    "\n",
    "        results_comments = session.run(\n",
    "            query_comments, developer=developer, start=start, end=end).data()\n",
    "        results_issues = session.run(\n",
    "            query_issues, developer=developer, start=start, end=end).data()\n",
    "        results_pullreq = session.run(\n",
    "            query_pullreq, developer=developer, start=start, end=end).data()\n",
    "        results_commits = session.run(\n",
    "            query_commits, developer=developer, start=start, end=end).data()\n",
    "\n",
    "        comments = pd.DataFrame.from_dict(results_comments)        \n",
    "        issues = pd.DataFrame.from_dict(results_issues)\n",
    "        pullreqs = pd.DataFrame.from_dict(results_pullreq)\n",
    "        commits = pd.DataFrame.from_dict(results_commits)\n",
    "\n",
    "        repositories = pd.concat([comments, issues, pullreqs, commits], axis=0, ignore_index=True)\n",
    "        repositories.drop_duplicates(subset=['repo_name'], inplace=True)\n",
    "\n",
    "    return repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_packages_by_repos(repos):\n",
    "    with driver.session(database='main') as session:\n",
    "        query = \"\"\"\n",
    "                MATCH (pa:Package)-[r:DEVELOPED_AT]->(pr:Project)\n",
    "                WHERE pr.id IN $repos\n",
    "                RETURN pa.name AS name,\n",
    "                       pa.repo_name AS repo_name,\n",
    "                       pa.repo_owner AS repo_owner,\n",
    "                       toString(pa.created) AS created\n",
    "                \"\"\"\n",
    "        \n",
    "        results = session.run(query, repos=repos).data()\n",
    "    \n",
    "        return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b90c6b5e784978a917bd71b9302405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_packages_by_dev_sample(developers):\n",
    "    repositories = []\n",
    "    # Get repositories developers contributed to during full observation\n",
    "    for dev in tqdm(developers):\n",
    "        dev_repos = get_developer_affiliations(dev, OBSERVATION_START, DATE_END)\n",
    "        repositories.extend(dev_repos['repo_name'].tolist())\n",
    "\n",
    "    repositories = list(set(repositories))\n",
    "\n",
    "    # Keep repositories with one package behind\n",
    "    packages = get_packages(driver, DATE_END)\n",
    "    selected_repos = [repo for repo in repositories if repo in packages['repo_name'].tolist()]\n",
    "    sample_packages = get_packages_by_repos(selected_repos)\n",
    "    nodelist_pkgs = [\"_\".join([\"pkg\", name]) for name in sample_packages['name'].tolist()]\n",
    "\n",
    "    nodelist = pd.DataFrame(nodelist_pkgs, columns=['id'])\n",
    "    nodelist.to_csv('../data/nodelists/packages.csv', index=False)\n",
    "\n",
    "    return sample_packages, nodelist_pkgs\n",
    "    \n",
    "sample_packages, nodelist_pkgs = get_packages_by_dev_sample(sample_devs['login'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version(driver, package, date):\n",
    "    with driver.session(database='main') as session:\n",
    "        query = \"\"\"\n",
    "                OPTIONAL MATCH (p:Package { id: $package })-[:RELEASED]->(v:Version)\n",
    "                WHERE v.created < DateTime($date)\n",
    "                AND NOT v.number CONTAINS \"-\"\n",
    "                RETURN p.name AS name,\n",
    "                       v.id AS version_id,\n",
    "                       v.number AS version,\n",
    "                       v.license AS license,\n",
    "                       toString(v.created) AS version_created\n",
    "                ORDER BY v.created DESC\n",
    "                LIMIT 1\n",
    "                \"\"\"\n",
    "        return session.run(query, package=package, date=date).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependencies(driver, version_ids):\n",
    "    with driver.session(database='main') as session:\n",
    "        query = \"\"\"\n",
    "                UNWIND $versions AS version\n",
    "                MATCH (v:Version { id: version })-[d:DEPENDS_ON]->(p:Package)\n",
    "                RETURN v.package_id AS source,\n",
    "                       p.id AS target,\n",
    "                       d.requirements AS requirements,\n",
    "                       toString(v.created) AS created\n",
    "                \"\"\"\n",
    "\n",
    "        results = session.run(query, versions=version_ids).data()\n",
    "\n",
    "        dependencies = pd.DataFrame.from_dict(results)\n",
    "        dependencies['created'] = pd.to_datetime(dependencies['created'])\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a13bb0874a84fd697fc935ed563e3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5706c690664313ab21090a9f8f3180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e1dc89591b43e5b1d5c5351bef50d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82507a4f9c94bbe9dc9569b7034d51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d32d1de89be4590ba64f4461f371c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869222ae22024d09960983a8ae9fd39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362344c99aab4fb3a49e2e17d7ee1eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b7cc64a72e4ec196c950ac0ac6a90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba84c05c704cd7a5a62d30b34475fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_dependency_networks():\n",
    "    # Create dependency networks for each observation\n",
    "    dependency_networks = []\n",
    "\n",
    "    with tqdm(total=len(observations)) as pbar:\n",
    "        for obs in observations:\n",
    "            packages = get_packages(driver, obs)\n",
    "\n",
    "            latest_versions = []\n",
    "            for package in tqdm(packages['name'].tolist(), leave=False):\n",
    "                latest_version = get_latest_version(driver, package, obs)\n",
    "                try:\n",
    "                    latest_versions.append(latest_version[0])\n",
    "                except KeyError:\n",
    "                    pass  # Package has no version\n",
    "\n",
    "            versions = pd.DataFrame.from_records(latest_versions)\n",
    "            versions['version_created'] = pd.to_datetime(versions['version_created'])\n",
    "\n",
    "            packages = packages.merge(versions, how=\"left\", on=['name'])\n",
    "            dependencies = get_dependencies(driver, packages['version_id'].tolist())\n",
    "\n",
    "            # Add prefix to match nodelist\n",
    "            dependencies['source'] = \"pkg_\" + dependencies['source']\n",
    "            dependencies['target'] = \"pkg_\" + dependencies['target']\n",
    "\n",
    "            edgelist = list(zip(dependencies['source'], dependencies['target']))\n",
    "\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from(nodelist_pkgs)\n",
    "            G.add_edges_from(edgelist)\n",
    "\n",
    "            dependency_networks.append(G)\n",
    "\n",
    "            nx.write_edgelist(G, '../data/edgelists/dependency_network-{0}.edgelist'.format(obs), delimiter=\",\", data=False)\n",
    "            nx.write_gpickle(G, '../data/networks/dependency_network-{0}.pkl'.format(obs))\n",
    "            nx.write_gml(G, '../data/networks/dependency_network-{0}.gml'.format(obs))\n",
    "\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "    return dependency_networks\n",
    "\n",
    "dependency_networks = create_dependency_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4fabd70eda43b0b7ac89a9e6fda663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_affiliation_networks():\n",
    "    # Create affiliation networks for each observation\n",
    "    affiliation_networks = []\n",
    "\n",
    "    with tqdm(total=len(observations)) as pbar:\n",
    "        for i, obs in enumerate(observations):\n",
    "            affiliations = []\n",
    "            for dev in sample_devs['login'].tolist():\n",
    "                if i == 0:\n",
    "                    dev_affiliations = get_developer_affiliations(dev, OBSERVATION_START, obs)\n",
    "                else:\n",
    "                    dev_affiliations = get_developer_affiliations(dev, observations[i-1], obs)\n",
    "                dev_affiliations['login'] = dev\n",
    "                affiliations.extend(dev_affiliations.to_dict(orient=\"records\"))\n",
    "\n",
    "            affiliations = pd.DataFrame.from_records(affiliations)\n",
    "            # Add package names to affiliation data\n",
    "            affiliations = affiliations.merge(sample_packages[[\"repo_name\", \"name\"]], how=\"left\", on=\"repo_name\")\n",
    "            \n",
    "            affiliations['source'] = \"dev_\" + affiliations['login']\n",
    "            affiliations['target'] = \"pkg_\" + affiliations['name']\n",
    "            \n",
    "            # Keep edges between devs and packages that are in nodelists\n",
    "            affiliations = affiliations[affiliations['source'].isin(nodelist_devs)]\n",
    "            affiliations = affiliations[affiliations['target'].isin(nodelist_pkgs)]\n",
    "\n",
    "            edgelist = list(zip(affiliations['source'], affiliations['target']))\n",
    "            \n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from(nodelist_devs, bipartite=0)\n",
    "            G.add_nodes_from(nodelist_pkgs, bipartite=1)\n",
    "            G.add_edges_from(edgelist)\n",
    "\n",
    "            affiliation_networks.append(G)\n",
    "\n",
    "            nx.write_edgelist(G, '../data/edgelists/affiliation_network-{0}.edgelist'.format(obs), delimiter=\",\", data=False)\n",
    "            nx.write_gpickle(G, '../data/networks/affiliation_network-{0}.pkl'.format(obs))\n",
    "            nx.write_gml(G, '../data/networks/affiliation_network-{0}.gml'.format(obs))\n",
    "\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "    return affiliation_networks\n",
    "\n",
    "affiliation_networks = create_affiliation_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3501d7f8304049ab86d069cd9446ab38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjacency list for SIENA\n",
    "def create_adjacency_matrix():\n",
    "    with tqdm(total=len(observations)) as pbar:\n",
    "        for i, obs in enumerate(observations):\n",
    "            adj = nx.bipartite.biadjacency_matrix(affiliation_networks[i], row_order=nodelist_devs, column_order=nodelist_pkgs)\n",
    "            am = adj.toarray()\n",
    "            np.savetxt('../data/adjacency/net-{0}.txt'.format(obs), am, fmt='%s')\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "create_adjacency_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_package_composition():\n",
    "    # Composition changes for dependency network\n",
    "    composition = sample_packages[['name', 'created']].copy()\n",
    "    composition['name'] = \"pkg_\" + composition['name']\n",
    "    composition['appearance'] = 0\n",
    "\n",
    "    rev_obs = sorted(observations,  reverse=True)\n",
    "    for i, item in enumerate(rev_obs):\n",
    "        obs = len(rev_obs) - i\n",
    "        composition.loc[composition['created'] < item, 'appearance'] = int(obs)\n",
    "\n",
    "    arr = []\n",
    "    for i, row in composition.iterrows():\n",
    "        curr = [int(row['appearance']), len(observations)]\n",
    "        arr.append(curr)\n",
    "        \n",
    "    comp_arr = np.array(arr)\n",
    "    np.savetxt('../data/compositions/pkgs_comp.txt', comp_arr, fmt='%d')\n",
    "\n",
    "create_package_composition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_developer_composition():\n",
    "    # Composition changes for dependency network\n",
    "    composition = sample_devs.copy()\n",
    "    composition['name'] = \"dev_\" + composition['login']\n",
    "    composition['appearance'] = 1\n",
    "    \n",
    "    inactive_developers = []\n",
    "    for i, item in enumerate(observations):\n",
    "         # Identify developers without outdegree in first period\n",
    "        devs_degrees = dict(affiliation_networks[i].out_degree(nodelist_devs))\n",
    "        devs_degrees = pd.DataFrame.from_dict(devs_degrees, orient=\"index\", columns=[\"out\"]).reset_index()\n",
    "        devs_degrees.rename(columns={\"index\": \"name\"}, inplace=True)\n",
    "\n",
    "        if i == 0: \n",
    "            inactive_developers.append(devs_degrees[devs_degrees['out'] == 0]['name'].tolist())\n",
    "        else:\n",
    "            devs = devs_degrees[devs_degrees['out'] == 0]['name'].tolist()\n",
    "            still_inactive_developers = [dev for dev in devs if dev in inactive_developers[i-1]]\n",
    "            inactive_developers.append(still_inactive_developers)\n",
    "    \n",
    "    for j, developers in enumerate(inactive_developers):\n",
    "        composition.loc[composition['name'].isin(developers), 'appearance'] = int(j + 2)\n",
    "\n",
    "    arr = []\n",
    "    for i, row in composition.iterrows():\n",
    "        curr = [int(row['appearance']), len(observations)]\n",
    "        arr.append(curr)\n",
    "        \n",
    "    comp_arr = np.array(arr)\n",
    "    np.savetxt('../data/compositions/devs_comp.txt', comp_arr, fmt='%d')\n",
    "\n",
    "create_developer_composition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define and think about what makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7c72e2a8b74534888a42254dd63177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_package_dependencies():\n",
    "    dependencies = sample_packages[['name', 'repo_name']].copy()\n",
    "    dependencies['name'] = \"pkg_\" + dependencies['name']\n",
    "\n",
    "    with tqdm(total=len(observations)) as pbar:\n",
    "        for i, obs in enumerate(observations):\n",
    "            idegree = dict(dependency_networks[i].in_degree())\n",
    "            odegree = dict(dependency_networks[i].out_degree())\n",
    "            \n",
    "            df_idegree = pd.DataFrame.from_dict(idegree, orient=\"index\", columns=[\"_\".join([\"in\", obs])]).reset_index()\n",
    "            df_idegree.rename(columns={\"index\": \"name\"}, inplace=True)\n",
    "\n",
    "            df_odegree = pd.DataFrame.from_dict(odegree, orient=\"index\", columns=[\"_\".join([\"out\", obs])]).reset_index()\n",
    "            df_odegree.rename(columns={\"index\": \"name\"}, inplace=True)\n",
    "\n",
    "            dependencies = dependencies.merge(df_idegree, how=\"left\", on=\"name\")\n",
    "            dependencies = dependencies.merge(df_odegree, how=\"left\", on=\"name\")\n",
    "\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "        \n",
    "    columns_in = [\"_\".join([\"in\", obs]) for obs in observations]\n",
    "    columns_out = [\"_\".join([\"out\", obs]) for obs in observations]\n",
    "\n",
    "    dependencies[columns_in] = dependencies[columns_in].astype(\"Int64\")\n",
    "    dependencies[columns_in] = dependencies[columns_in].astype(str)\n",
    "\n",
    "    dependencies[columns_out] = dependencies[columns_out].astype(\"Int64\")\n",
    "    dependencies[columns_out] = dependencies[columns_out].astype(str)\n",
    "\n",
    "    dependencies.replace(to_replace='<NA>', value='NA', inplace=True)\n",
    "\n",
    "    np.savetxt('../data/individual/pkg_upstream.txt', dependencies[columns_out].values, fmt='%s')\n",
    "    np.savetxt('../data/individual/pkg_downstream.txt', dependencies[columns_in].values, fmt='%s')\n",
    "    \n",
    "create_package_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyadic Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "833eb098fc81a387e667a83821bd3bfe54e47770c4f6f3f0b07bf099c925af38"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('posse-data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
